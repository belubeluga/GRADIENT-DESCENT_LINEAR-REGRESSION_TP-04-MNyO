# TP04 - M√©todos Num√©ricos y Optimizaci√≥n (MNyO) - Primer Semestre 2024

Este trabajo pr√°ctico aborda dos problemas cl√°sicos de optimizaci√≥n num√©rica utilizando el m√©todo de **gradiente descendente**:  
1. La minimizaci√≥n de la **funci√≥n de Rosenbrock** en 2D.  
2. La **regresi√≥n lineal por m√≠nimos cuadrados** aplicada al dataset California Housing.

üìÑ [Descargar informe TP04 en PDF](TP04_informe_MNyO.pdf.zip)

---

## üß™ PUNTO 1: Optimizaci√≥n en 2 dimensiones - Funci√≥n de Rosenbrock

Se implementa el **algoritmo de gradiente descendente** para minimizar la funci√≥n:


### Objetivos:

- Estudiar la **convergencia** del m√©todo en funci√≥n del **learning rate**.
- Analizar el impacto de diferentes **condiciones iniciales**.
- Visualizar trayectorias de descenso en el plano (x, y).
- (Opcional) Comparar con el **m√©todo de Newton**.

---

## üìä PUNTO 2: Cuadrados M√≠nimos por Gradiente Descendente

Se realiza una regresi√≥n lineal para predecir `MedHouseVal` en el dataset `California Housing` de `sklearn`.

### M√©todos implementados:

1. **Pseudoinversa**
2. **Gradiente descendente**
3. **Ridge Regression**

### An√°lisis:

- Comparaci√≥n de soluciones obtenidas por ambos m√©todos
- Evoluci√≥n del error en entrenamiento y testeo
- Sensibilidad frente al Œ∑ y regularizaci√≥n

---

## ‚úÖ Requisitos

- Python 3.x
- numpy
- matplotlib
- pandas
- scikit-learn
- Jupyter Notebook

---
